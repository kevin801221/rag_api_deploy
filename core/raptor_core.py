import os
import json
import time
import hashlib
import numpy as np
import pandas as pd
import tiktoken
import umap
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
from datetime import datetime
from sklearn.mixture import GaussianMixture

# LangChain ç›¸é—œå°å…¥
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

# å˜—è©¦å°å…¥å¯é¸çš„æ–‡æª”è¼‰å…¥å™¨
try:
    from langchain_community.document_loaders import UnstructuredWordDocumentLoader
    DOCX_SUPPORT = True
except ImportError:
    DOCX_SUPPORT = False


# å…¨åŸŸç‹€æ…‹å­—å…¸
_global_state = {
    'config': None,
    'embd': None,
    'model': None,
    'qdrant_client': None,
    'vectorstore': None,
    'rag_chain': None,
    'api_keys': [],
    'current_key_index': 0
}

# å¸¸æ•¸
RANDOM_SEED = 224


# ===============================================
# é…ç½®ç®¡ç†å‡½æ•¸
# ===============================================

def get_default_config() -> Dict:
    """ç²å–é è¨­é…ç½®"""
    return {
        "chunk_size": 1500,
        "chunk_overlap": 150,
        "n_levels": 3,
        "embedding_model": "text-embedding-3-small",
        "llm_model": "gpt-4o-mini",
        "retrieval_k": 6,
        "max_tokens_per_batch": 100000
    }


def init_raptor_core(config: Dict = None):
    """åˆå§‹åŒ– RAPTOR æ ¸å¿ƒ
    
    Args:
        config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹è¨­ç½®ã€åˆ†å¡Šåƒæ•¸ç­‰
    """
    global _global_state
    
    _global_state['config'] = config or get_default_config()
    _global_state['embd'] = None
    _global_state['model'] = None
    _global_state['qdrant_client'] = None
    _global_state['vectorstore'] = None
    _global_state['rag_chain'] = None
    _global_state['api_keys'] = []
    _global_state['current_key_index'] = 0
    
    print("ğŸš€ RAPTOR Core åˆå§‹åŒ–å®Œæˆ")


def get_config() -> Dict:
    """ç²å–ç•¶å‰é…ç½®"""
    return _global_state.get('config', get_default_config())


def update_config(new_config: Dict):
    """æ›´æ–°é…ç½®
    
    Args:
        new_config: æ–°çš„é…ç½®å­—å…¸
    """
    if _global_state['config']:
        _global_state['config'].update(new_config)
    else:
        _global_state['config'] = new_config
    print("âœ… é…ç½®å·²æ›´æ–°")


def print_config():
    """æ‰“å°ç•¶å‰é…ç½®"""
    config = get_config()
    print("\nğŸ“‹ RAPTOR Core é…ç½®:")
    print("=" * 40)
    for key, value in config.items():
        print(f"{key}: {value}")
    print("=" * 40)


# ===============================================
# API Key ç®¡ç†å‡½æ•¸
# ===============================================

def load_api_keys_from_files() -> List[str]:
    """å¾å„ç¨®é…ç½®æ–‡ä»¶è¼‰å…¥ API Keys
    
    Returns:
        List[str]: API Key åˆ—è¡¨
    """
    api_keys = []
    
    # å¾ç’°å¢ƒè®Šé‡è¼‰å…¥
    for i in range(1, 10):
        env_key = os.getenv(f"OPENAI_API_KEY_{i}")
        if env_key:
            api_keys.append(env_key)
    
    env_key = os.getenv('OPENAI_API_KEY')
    if env_key and env_key not in api_keys:
        api_keys.append(env_key)
    
    # å¾ JSON æ–‡ä»¶è¼‰å…¥
    if os.path.exists('api_keys.json'):
        try:
            with open('api_keys.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                if 'openai_api_keys' in data:
                    for key in data['openai_api_keys']:
                        if key and key not in api_keys:
                            api_keys.append(key)
        except Exception as e:
            print(f"âš ï¸ è®€å– api_keys.json å¤±æ•—: {e}")
    
    # å¾æ–‡æœ¬æ–‡ä»¶è¼‰å…¥
    if os.path.exists('openai_api_keys.txt'):
        try:
            with open('openai_api_keys.txt', 'r', encoding='utf-8') as f:
                keys = [line.strip() for line in f.readlines() 
                       if line.strip() and not line.startswith('#')]
                for key in keys:
                    if key not in api_keys:
                        api_keys.append(key)
        except Exception as e:
            print(f"âš ï¸ è®€å– openai_api_keys.txt å¤±æ•—: {e}")
    
    # å»é‡
    unique_keys = []
    seen = set()
    for key in api_keys:
        if key and key.strip() and key not in seen:
            unique_keys.append(key)
            seen.add(key)
    
    return unique_keys


def rotate_api_key() -> bool:
    """è¼ªèª¿ API Key"""
    global _global_state
    
    api_keys = _global_state.get('api_keys', [])
    current_index = _global_state.get('current_key_index', 0)
    
    if len(api_keys) > 1:
        new_index = (current_index + 1) % len(api_keys)
        new_key = api_keys[new_index]
        os.environ["OPENAI_API_KEY"] = new_key
        
        try:
            config = get_config()
            _global_state['embd'] = OpenAIEmbeddings(model=config['embedding_model'])
            _global_state['current_key_index'] = new_index
            print(f"ğŸ”„ åˆ‡æ›åˆ° API Key #{new_index + 1}")
            return True
        except Exception as e:
            print(f"âš ï¸ åˆ‡æ› API Key å¤±æ•—: {e}")
            return False
    return False

# æ¨¡å‹è¨­ç½®å‡½æ•¸

def setup_models(openai_api_key: str = None, 
                openai_api_keys: List[str] = None) -> bool:
    """è¨­ç½® OpenAI æ¨¡å‹
    
    Args:
        openai_api_key: å–®å€‹ API Key
        openai_api_keys: å¤šå€‹ API Key åˆ—è¡¨
        
    Returns:
        bool: è¨­ç½®æ˜¯å¦æˆåŠŸ
    """
    global _global_state
    
    try:
        # è¨­ç½® API Keys
        if openai_api_keys:
            api_keys = [key for key in openai_api_keys if key and key.strip()]
            if api_keys:
                _global_state['api_keys'] = api_keys
                os.environ["OPENAI_API_KEY"] = api_keys[0]
                print(f"âœ… è¨­ç½®äº† {len(api_keys)} å€‹ OpenAI API Keysï¼Œæ”¯æ´è¼ªèª¿")
            else:
                raise ValueError("æ‰€æœ‰ API Key éƒ½æ˜¯ç©ºçš„")
        elif openai_api_key:
            if openai_api_key and openai_api_key.strip():
                _global_state['api_keys'] = [openai_api_key.strip()]
                os.environ["OPENAI_API_KEY"] = openai_api_key.strip()
                print("âœ… è¨­ç½®äº† 1 å€‹ OpenAI API Key")
            else:
                raise ValueError("API Key æ˜¯ç©ºçš„")
        else:
            raise ValueError("æ²’æœ‰æä¾› API Key")
        
        config = get_config()
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        _global_state['embd'] = OpenAIEmbeddings(model=config['embedding_model'])
        print(f"âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {config['embedding_model']}")
        
        # åˆå§‹åŒ–èªè¨€æ¨¡å‹
        _global_state['model'] = ChatOpenAI(
            temperature=0,
            model=config['llm_model']
        )
        print(f"âœ… èªè¨€æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {config['llm_model']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¨­ç½®å¤±æ•—: {e}")
        return False

# Qdrant è¨­ç½®å‡½æ•¸

def setup_qdrant(qdrant_url: str,
                qdrant_api_key: str,
                collection_name: str = "rag_knowledge",
                force_recreate: bool = False) -> bool:
    """è¨­ç½® Qdrant å‘é‡è³‡æ–™åº«
    
    Args:
        qdrant_url: Qdrant æœå‹™ URL
        qdrant_api_key: Qdrant API Key
        collection_name: é›†åˆåç¨±
        force_recreate: æ˜¯å¦å¼·åˆ¶é‡å»ºé›†åˆ
        
    Returns:
        bool: è¨­ç½®æ˜¯å¦æˆåŠŸ
    """
    global _global_state
    
    try:
        # é€£æ¥åˆ° Qdrant
        _global_state['qdrant_client'] = QdrantClient(
            url=qdrant_url,
            api_key=qdrant_api_key
        )
        print(f"âœ… æˆåŠŸé€£æ¥åˆ° Qdrant: {qdrant_url}")
        
        # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        collections = _global_state['qdrant_client'].get_collections().collections
        collection_exists = any(c.name == collection_name for c in collections)
        
        if force_recreate and collection_exists:
            print(f"ğŸ—‘ï¸ åˆªé™¤ç¾æœ‰é›†åˆ: {collection_name}")
            _global_state['qdrant_client'].delete_collection(collection_name)
            collection_exists = False
        
        # å‰µå»ºé›†åˆï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not collection_exists:
            print(f"ğŸ†• å‰µå»ºæ–°é›†åˆ: {collection_name}")
            _global_state['qdrant_client'].create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=1536,  # text-embedding-3-small çš„ç¶­åº¦
                    distance=Distance.COSINE
                )
            )
        else:
            print(f"ğŸ“‹ ä½¿ç”¨ç¾æœ‰é›†åˆ: {collection_name}")
        
        # åˆå§‹åŒ–å‘é‡å­˜å„²
        _global_state['vectorstore'] = QdrantVectorStore(
            client=_global_state['qdrant_client'],
            collection_name=collection_name,
            embedding=_global_state['embd']
        )
        print("âœ… Qdrant å‘é‡è³‡æ–™åº«è¨­ç½®å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ Qdrant è¨­ç½®å¤±æ•—: {e}")
        return False

# Token è¨ˆç®—å‡½æ•¸

def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    """è¨ˆç®—å­—ä¸²çš„ token æ•¸é‡"""
    try:
        encoding = tiktoken.get_encoding(encoding_name)
        return len(encoding.encode(string))
    except Exception:
        # å¦‚æœ tiktoken å¤±æ•—ï¼Œä½¿ç”¨ç²—ç•¥ä¼°è¨ˆ
        return int(len(string.split()) * 1.3)

# RAPTOR èšé¡ç®—æ³•å‡½æ•¸

def global_cluster_embeddings(embeddings: np.ndarray, 
                             dim: int,
                             n_neighbors: Optional[int] = None,
                             metric: str = "cosine") -> np.ndarray:
    """å…¨å±€èšé¡åµŒå…¥"""
    try:
        if n_neighbors is None:
            n_neighbors = max(2, min(50, int((len(embeddings) - 1) ** 0.5)))
        
        return umap.UMAP(
            n_neighbors=n_neighbors, 
            n_components=dim, 
            metric=metric,
            random_state=RANDOM_SEED
        ).fit_transform(embeddings)
    except Exception as e:
        print(f"âš ï¸ å…¨å±€èšé¡å¤±æ•—: {e}")
        return embeddings[:, :dim] if embeddings.shape[1] >= dim else embeddings

def local_cluster_embeddings(embeddings: np.ndarray,
                            dim: int,
                            num_neighbors: int = 10,
                            metric: str = "cosine") -> np.ndarray:
    """å±€éƒ¨èšé¡åµŒå…¥"""
    try:
        num_neighbors = max(2, min(num_neighbors, len(embeddings) - 1))
        return umap.UMAP(
            n_neighbors=num_neighbors, 
            n_components=dim, 
            metric=metric,
            random_state=RANDOM_SEED
        ).fit_transform(embeddings)
    except Exception as e:
        print(f"âš ï¸ å±€éƒ¨èšé¡å¤±æ•—: {e}")
        return embeddings[:, :dim] if embeddings.shape[1] >= dim else embeddings

def get_optimal_clusters(embeddings: np.ndarray, max_clusters: int = 20) -> int:
    """ä½¿ç”¨ BIC ç²å–æœ€ä½³èšé¡æ•¸é‡"""
    try:
        max_clusters = min(max_clusters, len(embeddings), 20)
        if max_clusters <= 1:
            return 1
            
        n_clusters = np.arange(1, max_clusters + 1)
        bics = []
        for n in n_clusters:
            try:
                gm = GaussianMixture(n_components=n, random_state=RANDOM_SEED)
                gm.fit(embeddings)
                bics.append(gm.bic(embeddings))
            except Exception:
                bics.append(float('inf'))
        
        return n_clusters[np.argmin(bics)]
    except Exception:
        return min(3, len(embeddings))

def GMM_cluster(embeddings: np.ndarray, threshold: float = 0.1):
    """é«˜æ–¯æ··åˆæ¨¡å‹èšé¡"""
    try:
        n_clusters = get_optimal_clusters(embeddings)
        gm = GaussianMixture(n_components=n_clusters, random_state=RANDOM_SEED)
        gm.fit(embeddings)
        probs = gm.predict_proba(embeddings)
        labels = [np.where(prob > threshold)[0] for prob in probs]
        return labels, n_clusters
    except Exception as e:
        print(f"âš ï¸ GMM èšé¡å¤±æ•—: {e}")
        n_clusters = min(3, len(embeddings))
        labels = [np.array([i % n_clusters]) for i in range(len(embeddings))]
        return labels, n_clusters

def perform_clustering(embeddings: np.ndarray, 
                      dim: int = 10, 
                      threshold: float = 0.1) -> List[np.ndarray]:
    """åŸ·è¡Œèšé¡"""
    try:
        if len(embeddings) <= 2:
            return [np.array([0]) for _ in range(len(embeddings))]
        
        reduced_embeddings = global_cluster_embeddings(
            embeddings, 
            min(dim, embeddings.shape[1])
        )
        labels, n_clusters = GMM_cluster(reduced_embeddings, threshold)
        
        return labels
    except Exception as e:
        print(f"âš ï¸ èšé¡éç¨‹å¤±æ•—: {e}")
        return [np.array([i % 3]) for i in range(len(embeddings))]

# åµŒå…¥è™•ç†å‡½æ•¸

def embed_batch_with_retry(texts: List[str], max_retries: int = 3) -> List[List[float]]:
    """å¸¶é‡è©¦å’Œ API Key è¼ªèª¿çš„æ‰¹æ¬¡åµŒå…¥"""
    embd = _global_state.get('embd')
    if not embd:
        print("âŒ åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
        return [np.zeros(1536).tolist() for _ in texts]
    
    for attempt in range(max_retries):
        try:
            embeddings = embd.embed_documents(texts)
            return embeddings
        
        except Exception as e:
            error_msg = str(e)
            print(f"   âš ï¸ åµŒå…¥å¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {error_msg}")
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯ token é™åˆ¶éŒ¯èª¤
            if "max_tokens_per_request" in error_msg or "too long" in error_msg.lower():
                print("   ğŸ“‰ Token æ•¸é‡éå¤šï¼Œåˆ†å‰²æ‰¹æ¬¡...")
                mid = len(texts) // 2
                if mid > 0:
                    part1 = embed_batch_with_retry(texts[:mid], max_retries - attempt)
                    part2 = embed_batch_with_retry(texts[mid:], max_retries - attempt)
                    return part1 + part2
                else:
                    print(f"  âŒ è·³ééå¤§çš„æ–‡æœ¬")
                    return [np.zeros(1536).tolist()]
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯ API é™åˆ¶éŒ¯èª¤
            elif any(keyword in error_msg.lower() for keyword in ["rate_limit", "quota", "limit"]):
                if rotate_api_key():
                    time.sleep(2)
                    continue
            
            # å…¶ä»–éŒ¯èª¤ï¼Œç­‰å¾…å¾Œé‡è©¦
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                print(f"   â±ï¸ ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
    
    print(f"   âŒ æ‰¹æ¬¡åµŒå…¥æœ€çµ‚å¤±æ•—ï¼Œè¿”å›é›¶å‘é‡")
    return [np.zeros(1536).tolist() for _ in texts]


def embed_texts(texts: List[str]) -> np.ndarray:
    """å°‡æ–‡æœ¬è½‰æ›ç‚ºåµŒå…¥å‘é‡
    
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        
    Returns:
        np.ndarray: åµŒå…¥å‘é‡æ•¸çµ„
    """
    if not texts:
        return np.array([])
    
    config = get_config()
    all_embeddings = []
    current_batch = []
    current_tokens = 0
    max_tokens_per_batch = config['max_tokens_per_batch']
    
    print(f"ğŸ“Š é–‹å§‹è™•ç† {len(texts)} å€‹æ–‡æœ¬çš„åµŒå…¥...")
    
    for i, text in enumerate(texts):
        text_tokens = num_tokens_from_string(str(text))
        
        if current_tokens + text_tokens > max_tokens_per_batch and current_batch:
            print(f"   è™•ç†æ‰¹æ¬¡ (å¤§å°: {len(current_batch)}, tokens: {current_tokens:,})")
            batch_embeddings = embed_batch_with_retry(current_batch)
            all_embeddings.extend(batch_embeddings)
            
            current_batch = [text]
            current_tokens = text_tokens
        else:
            current_batch.append(text)
            current_tokens += text_tokens
        
        if (i + 1) % 20 == 0:
            print(f"   é€²åº¦: {i + 1}/{len(texts)}")
    
    # è™•ç†æœ€å¾Œä¸€å€‹æ‰¹æ¬¡
    if current_batch:
        print(f"   è™•ç†æœ€å¾Œæ‰¹æ¬¡ (å¤§å°: {len(current_batch)}, tokens: {current_tokens:,})")
        batch_embeddings = embed_batch_with_retry(current_batch)
        all_embeddings.extend(batch_embeddings)
    
    print(f"âœ… åµŒå…¥è™•ç†å®Œæˆï¼Œç¸½å…± {len(all_embeddings)} å€‹å‘é‡")
    return np.array(all_embeddings)


def embed_cluster_texts(texts: List[str]) -> pd.DataFrame:
    """åµŒå…¥ä¸¦èšé¡æ–‡æœ¬"""
    try:
        text_embeddings_np = embed_texts(texts)
        if len(text_embeddings_np) == 0:
            return pd.DataFrame()
        
        cluster_labels = perform_clustering(text_embeddings_np, 10, 0.1)
        
        df = pd.DataFrame()
        df["text"] = texts
        df["embd"] = list(text_embeddings_np)
        df["cluster"] = cluster_labels
        return df
    except Exception as e:
        print(f"âš ï¸ åµŒå…¥èšé¡å¤±æ•—: {e}")
        df = pd.DataFrame()
        df["text"] = texts
        df["embd"] = [np.zeros(1536).tolist() for _ in texts]
        df["cluster"] = [np.array([0]) for _ in texts]
        return df

# æ–‡æœ¬æ ¼å¼åŒ–å’Œæ‘˜è¦å‡½æ•¸

def fmt_txt(df: pd.DataFrame) -> str:
    """æ ¼å¼åŒ–æ–‡æœ¬"""
    if df.empty:
        return ""
    unique_txt = df["text"].tolist()
    return "\n\n".join(unique_txt)

def embed_cluster_summarize_texts(texts: List[str], level: int) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """åµŒå…¥ã€èšé¡ä¸¦æ‘˜è¦æ–‡æœ¬"""
    model = _global_state.get('model')
    if not model:
        print("âŒ èªè¨€æ¨¡å‹æœªåˆå§‹åŒ–")
        return pd.DataFrame(), pd.DataFrame()
    
    try:
        df_clusters = embed_cluster_texts(texts)
        
        if df_clusters.empty:
            print(f"--ç¬¬ {level} å±¤è™•ç†å¤±æ•—ï¼Œè¿”å›ç©ºçµæœ--")
            return pd.DataFrame(), pd.DataFrame()
        
        expanded_list = []
        for index, row in df_clusters.iterrows():
            for cluster in row["cluster"]:
                expanded_list.append({
                    "text": row["text"],
                    "embd": row["embd"],
                    "cluster": cluster
                })
        
        if not expanded_list:
            print(f"--ç¬¬ {level} å±¤æ²’æœ‰æœ‰æ•ˆèšé¡--")
            return df_clusters, pd.DataFrame()
        
        expanded_df = pd.DataFrame(expanded_list)
        all_clusters = expanded_df["cluster"].unique()
        
        print(f"--ç¬¬ {level} å±¤ç”Ÿæˆ {len(all_clusters)} å€‹èšé¡--")
        
        template = """é€™æ˜¯ä¸€ä»½æ–‡æª”çš„å­é›†ã€‚è«‹ç‚ºæä¾›çš„æ–‡æª”å…§å®¹çµ¦å‡ºç°¡æ½”çš„æ‘˜è¦ã€‚

æ–‡æª”å…§å®¹:
{context}

æ‘˜è¦:"""
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | model | StrOutputParser()
        
        summaries = []
        for i in all_clusters:
            try:
                df_cluster = expanded_df[expanded_df["cluster"] == i]
                formatted_txt = fmt_txt(df_cluster)
                if formatted_txt.strip():
                    summary = chain.invoke({"context": formatted_txt})
                    summaries.append(summary)
                else:
                    summaries.append(f"ç©ºèšé¡ {i}")
            except Exception as e:
                print(f"   âš ï¸ èšé¡ {i} æ‘˜è¦å¤±æ•—: {e}")
                summaries.append(f"èšé¡ {i} æ‘˜è¦å¤±æ•—")
        
        df_summary = pd.DataFrame({
            "summaries": summaries,
            "level": [level] * len(summaries),
            "cluster": list(all_clusters),
        })
        
        return df_clusters, df_summary
        
    except Exception as e:
        print(f"âš ï¸ ç¬¬ {level} å±¤è™•ç†å¤±æ•—: {e}")
        return pd.DataFrame(), pd.DataFrame()

def recursive_embed_cluster_summarize(texts: List[str], level: int = 1) -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:
    """éæ­¸åµŒå…¥èšé¡æ‘˜è¦ - RAPTOR æ ¸å¿ƒç®—æ³•"""
    config = get_config()
    results = {}
    
    try:
        df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)
        results[level] = (df_clusters, df_summary)
        
        # æª¢æŸ¥æ˜¯å¦ç¹¼çºŒä¸‹ä¸€å±¤
        if (level < config['n_levels'] and 
            not df_summary.empty and 
            df_summary["cluster"].nunique() > 1 and
            len(df_summary) > 1):
            
            new_texts = df_summary["summaries"].tolist()
            if new_texts and any(text.strip() for text in new_texts):
                next_level_results = recursive_embed_cluster_summarize(
                    new_texts, level + 1
                )
                results.update(next_level_results)
    
    except Exception as e:
        print(f"âš ï¸ ç¬¬ {level} å±¤éæ­¸è™•ç†å¤±æ•—: {e}")
    
    return results

# æ–‡æª”è¼‰å…¥å‡½æ•¸

def load_documents_from_directory(directory_path: str) -> List:
    """å¾ç›®éŒ„è¼‰å…¥å„ç¨®æ ¼å¼çš„æ–‡æª”"""
    documents = []
    
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    directory = Path(directory_path)
    if not directory.exists():
        print(f"ğŸ“ ç›®éŒ„ä¸å­˜åœ¨: {directory_path}")
        return []
    
    # è¼‰å…¥ PDF
    try:
        pdf_loader = DirectoryLoader(
            directory_path, glob="**/*.pdf", loader_cls=PyPDFLoader
        )
        pdf_docs = pdf_loader.load()
        documents.extend(pdf_docs)
        print(f"ğŸ“„ è¼‰å…¥ {len(pdf_docs)} å€‹ PDF æ–‡ä»¶")
    except Exception as e:
        print(f"âš ï¸ PDF è¼‰å…¥éŒ¯èª¤: {e}")
    
    # è¼‰å…¥æ–‡æœ¬æ–‡ä»¶
    try:
        txt_loader = DirectoryLoader(
            directory_path, glob="**/*.txt", loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'}
        )
        txt_docs = txt_loader.load()
        documents.extend(txt_docs)
        print(f"ğŸ“ è¼‰å…¥ {len(txt_docs)} å€‹ TXT æ–‡ä»¶")
    except Exception as e:
        print(f"âš ï¸ TXT è¼‰å…¥éŒ¯èª¤: {e}")
    
    # è¼‰å…¥ Word æ–‡ä»¶ (å¦‚æœæ”¯æ´)
    if DOCX_SUPPORT:
        try:
            doc_loader = DirectoryLoader(
                directory_path, glob="**/*.docx",
                loader_cls=UnstructuredWordDocumentLoader
            )
            doc_docs = doc_loader.load()
            documents.extend(doc_docs)
            print(f"ğŸ“„ è¼‰å…¥ {len(doc_docs)} å€‹ DOCX æ–‡ä»¶")
        except Exception as e:
            print(f"âš ï¸ DOCX è¼‰å…¥éŒ¯èª¤: {e}")
    
    return documents

def load_and_split_file(file_path: str) -> List[str]:
    """è¼‰å…¥ä¸¦åˆ†å‰²å–®å€‹æ–‡ä»¶"""
    config = get_config()
    
    try:
        # æ ¹æ“šæ–‡ä»¶é¡å‹é¸æ“‡åˆé©çš„è¼‰å…¥å™¨
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file_path.endswith('.txt'):
            loader = TextLoader(file_path, encoding='utf-8')
        elif file_path.endswith('.docx') and DOCX_SUPPORT:
            loader = UnstructuredWordDocumentLoader(file_path)
        else:
            print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶é¡å‹: {file_path}")
            return []
        
        # è¼‰å…¥æ–‡æª”
        docs = loader.load()
        if not docs:
            print(f"âš ï¸ æ–‡ä»¶ç‚ºç©º: {file_path}")
            return []
        
        # åˆä½µæ‰€æœ‰é é¢çš„å…§å®¹
        all_content = []
        for doc in docs:
            if hasattr(doc, 'page_content') and doc.page_content:
                all_content.append(doc.page_content)
        
        if not all_content:
            return []
        
        # åˆ†å‰²æ–‡æª”
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config['chunk_size'],
            chunk_overlap=config['chunk_overlap'],
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        concatenated_content = "\n\n".join(all_content)
        chunks = text_splitter.split_text(concatenated_content)
        
        print(f"ğŸ“„ å¾ {file_path} åˆ†å‰²å‡º {len(chunks)} å€‹ç‰‡æ®µ")
        return chunks
        
    except Exception as e:
        print(f"âš ï¸ è¼‰å…¥æ–‡ä»¶å¤±æ•— {file_path}: {e}")
        return []

# RAPTOR æ–‡æª”è™•ç†å‡½æ•¸

def process_documents_with_raptor(documents: List, file_hash: str = None) -> List[str]:
    """ä½¿ç”¨ RAPTOR ç®—æ³•è™•ç†æ–‡æª”
    
    Args:
        documents: æ–‡æª”åˆ—è¡¨
        file_hash: æ–‡ä»¶å“ˆå¸Œå€¼ï¼ˆç”¨æ–¼å…ƒæ•¸æ“šï¼‰
        
    Returns:
        List[str]: è™•ç†å¾Œçš„æ‰€æœ‰æ–‡æœ¬ç¯€é»
    """
    config = get_config()
    print("\nğŸŒ³ é–‹å§‹ RAPTOR è™•ç†...")
    
    # 1. æ–‡æª”å…§å®¹åˆä½µ
    all_content = []
    for doc in documents:
        if hasattr(doc, 'page_content') and doc.page_content:
            all_content.append(doc.page_content)
    
    if not all_content:
        print("âŒ æ–‡æª”å…§å®¹ç‚ºç©º")
        return []
    
    concatenated_content = "\n\n\n --- \n\n\n".join(all_content)
    total_tokens = num_tokens_from_string(concatenated_content)
    print(f"ğŸ“Š ç¸½ token æ•¸: {total_tokens:,}")
    
    # 2. æ–‡æœ¬åˆ†å‰²
    print("âœ‚ï¸ åˆ†å‰²æ–‡æœ¬...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config['chunk_size'],
        chunk_overlap=config['chunk_overlap']
    )
    texts_split = text_splitter.split_text(concatenated_content)
    print(f"ğŸ“‹ åˆ†å‰²æˆ {len(texts_split)} å€‹æ–‡æœ¬å¡Š")
    
    if not texts_split:
        print("âŒ æ–‡æœ¬åˆ†å‰²çµæœç‚ºç©º")
        return []
    
    # 3. RAPTOR æ¨¹æ§‹å»º
    print("ğŸŒ³ å»ºç«‹ RAPTOR æ¨¹çµæ§‹...")
    all_texts = texts_split.copy()
    
    try:
        results = recursive_embed_cluster_summarize(texts_split, level=1)
        
        # æ”¶é›†æ‰€æœ‰æ‘˜è¦
        for level in sorted(results.keys()):
            df_clusters, df_summary = results[level]
            if not df_summary.empty and 'summaries' in df_summary.columns:
                summaries = df_summary["summaries"].tolist()
                valid_summaries = [s for s in summaries if s and str(s).strip()]
                all_texts.extend(valid_summaries)
                print(f"   ç¬¬ {level} å±¤æ·»åŠ  {len(valid_summaries)} å€‹æ‘˜è¦")
    
    except Exception as e:
        print(f"âš ï¸ RAPTOR æ¨¹æ§‹å»ºéƒ¨åˆ†å¤±æ•—: {e}")
        print("   å°‡ä½¿ç”¨åŸºæœ¬æ–‡æœ¬å¡Š")
    
    print(f"ğŸ“š RAPTOR è™•ç†å®Œæˆï¼Œç¸½å…± {len(all_texts)} å€‹æ–‡æœ¬ç¯€é»")
    return all_texts


def process_single_file_with_raptor(file_path: str) -> Tuple[List[str], str]:
    """ä½¿ç”¨ RAPTOR è™•ç†å–®å€‹æ–‡ä»¶
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        
    Returns:
        Tuple[List[str], str]: (è™•ç†å¾Œçš„æ–‡æœ¬åˆ—è¡¨, æ–‡ä»¶å“ˆå¸Œå€¼)
    """
    print(f"\nğŸ“„ è™•ç†æ–‡ä»¶: {file_path}")
    
    # è¨ˆç®—æ–‡ä»¶å“ˆå¸Œ
    file_hash = calculate_file_hash(file_path)
    if not file_hash:
        return [], ""
    
    # è¼‰å…¥ä¸¦åˆ†å‰²æ–‡ä»¶
    text_chunks = load_and_split_file(file_path)
    if not text_chunks:
        return [], file_hash
    
    # ä½¿ç”¨ RAPTOR è™•ç†
    try:
        print("ğŸŒ³ é–‹å§‹ RAPTOR è™•ç†...")
        all_texts = text_chunks.copy()
        
        # ç°¡åŒ–çš„ RAPTOR è™•ç† - åªå°è¼ƒå¤§çš„æ–‡ä»¶é›†åˆé€²è¡Œèšé¡
        if len(text_chunks) > 5:
            results = recursive_embed_cluster_summarize(text_chunks, level=1)
            
            # æ”¶é›†æ‘˜è¦
            for level in sorted(results.keys()):
                df_clusters, df_summary = results[level]
                if not df_summary.empty and 'summaries' in df_summary.columns:
                    summaries = df_summary["summaries"].tolist()
                    valid_summaries = [s for s in summaries if s and str(s).strip()]
                    all_texts.extend(valid_summaries)
                    print(f"   ç¬¬ {level} å±¤æ·»åŠ  {len(valid_summaries)} å€‹æ‘˜è¦")
        
        print(f"âœ… RAPTOR è™•ç†å®Œæˆï¼Œç¸½å…± {len(all_texts)} å€‹æ–‡æœ¬ç¯€é»")
        return all_texts, file_hash
        
    except Exception as e:
        print(f"âš ï¸ RAPTOR è™•ç†å¤±æ•—: {e}")
        return text_chunks, file_hash  # å›é€€åˆ°åŸºæœ¬æ–‡æœ¬å¡Š

# å‘é‡å­˜å„²å‡½æ•¸

def add_texts_to_vectorstore(texts: List[str], metadata_list: List[Dict] = None) -> bool:
    """å°‡æ–‡æœ¬æ·»åŠ åˆ°å‘é‡å­˜å„²
    
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        metadata_list: å…ƒæ•¸æ“šåˆ—è¡¨
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    vectorstore = _global_state.get('vectorstore')
    if not vectorstore:
        print("âŒ å‘é‡å­˜å„²æœªåˆå§‹åŒ–")
        return False
    
    if not texts:
        print("âš ï¸ æ²’æœ‰æ–‡æœ¬éœ€è¦æ·»åŠ ")
        return True
    
    try:
        batch_size = 20
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_metadata = metadata_list[i:i+batch_size] if metadata_list else None
            
            # éæ¿¾ç©ºæ–‡æœ¬
            valid_indices = [j for j, text in enumerate(batch_texts) if text and str(text).strip()]
            valid_batch_texts = [batch_texts[j] for j in valid_indices]
            valid_batch_metadata = [batch_metadata[j] for j in valid_indices] if batch_metadata else None
            
            if valid_batch_texts:
                # ç¢ºä¿å…ƒæ•¸æ“šæ ¼å¼æ­£ç¢ºï¼ˆLangChain æœƒè‡ªå‹•åŒ…è£åˆ° metadata å­—æ®µä¸­ï¼‰
                if valid_batch_metadata:
                    # LangChain çš„ add_texts æœƒè‡ªå‹•å°‡ metadatas åŒ…è£åˆ° payload.metadata ä¸­
                    vectorstore.add_texts(valid_batch_texts, metadatas=valid_batch_metadata)
                else:
                    vectorstore.add_texts(valid_batch_texts)
                print(f"   å·²è™•ç† {min(i+batch_size, len(texts))}/{len(texts)} å€‹æ–‡æª”")
        
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(texts)} å€‹æ–‡æœ¬åˆ°å‘é‡å­˜å„²")
        return True
        
    except Exception as e:
        print(f"âŒ æ·»åŠ æ–‡æœ¬åˆ°å‘é‡å­˜å„²å¤±æ•—: {e}")
        return False

# RAG æŸ¥è©¢éˆå‡½æ•¸

def build_rag_chain() -> bool:
    """å»ºç«‹ RAG æŸ¥è©¢éˆ"""
    vectorstore = _global_state.get('vectorstore')
    model = _global_state.get('model')
    config = get_config()
    
    if not vectorstore:
        print("âŒ å‘é‡å­˜å„²æœªåˆå§‹åŒ–")
        return False
    
    if not model:
        print("âŒ èªè¨€æ¨¡å‹æœªåˆå§‹åŒ–")
        return False
    
    try:
        retriever = vectorstore.as_retriever(
            search_kwargs={"k": config['retrieval_k']}
        )
        
        prompt = ChatPromptTemplate.from_template(
            """æ ¹æ“šä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚è«‹æä¾›è©³ç´°ã€æº–ç¢ºçš„ç­”æ¡ˆã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºã€‚

ä¸Šä¸‹æ–‡: {context}

å•é¡Œ: {question}

å›ç­”:"""
        )
        
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        _global_state['rag_chain'] = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | model
            | StrOutputParser()
        )
        
        print("âœ… RAG æŸ¥è©¢éˆå»ºç«‹æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ RAG æŸ¥è©¢éˆå»ºç«‹å¤±æ•—: {e}")
        return False


def ask_question(question: str, retrieval_k: int = None, score_threshold: float = None) -> Tuple[str, List[Tuple[Any, float]]]:
    """å‘ RAG ç³»çµ±æå•ï¼Œä¸¦è¿”å›ç­”æ¡ˆå’Œç›¸é—œæ–‡æª”åŠåˆ†æ•¸
    
    Args:
        question: å•é¡Œ
        retrieval_k: æª¢ç´¢çµæœæ•¸é‡
        score_threshold: æª¢ç´¢åˆ†æ•¸é–¾å€¼
        
    Returns:
        Tuple[str, List[Tuple[Any, float]]]: (å›ç­”, ç›¸é—œæ–‡æª”åŠåˆ†æ•¸åˆ—è¡¨)
    """
    rag_chain = _global_state.get('rag_chain')
    vectorstore = _global_state.get('vectorstore')
    config = get_config()
    
    if not rag_chain:
        print("âŒ RAG æŸ¥è©¢éˆæœªåˆå§‹åŒ–ï¼Œè«‹å…ˆå»ºç«‹ RAG éˆ")
        return "", []
    
    print(f"â“ å•é¡Œ: {question}")
    print("ğŸ¤” æ€è€ƒä¸­...")
    
    try:
        # ç²å–ç›¸é—œæ–‡æª”åŠåˆ†æ•¸
        relevant_docs_with_scores = []
        if vectorstore:
            try:
                search_kwargs = {"k": retrieval_k if retrieval_k is not None else config['retrieval_k']}
                if score_threshold is not None:
                    search_kwargs["score_threshold"] = score_threshold
                
                relevant_docs_with_scores = vectorstore.similarity_search_with_score(
                    question,
                    **search_kwargs
                )
                print(f"ğŸ“š æ‰¾åˆ° {len(relevant_docs_with_scores)} å€‹ç›¸é—œæ–‡æª”ç‰‡æ®µ")
            except Exception as e:
                print(f"âš ï¸ æª¢ç´¢ç›¸é—œæ–‡æª”å¤±æ•—: {e}")
        
        # ç”Ÿæˆå›ç­”
        # æ³¨æ„ï¼šrag_chain é æœŸçš„æ˜¯ä¸€å€‹ retrieverï¼Œé€™è£¡éœ€è¦èª¿æ•´
        # ç‚ºäº†ä¿æŒ ask_question çš„ç°¡æ½”æ€§ï¼Œæˆ‘å€‘è®“å®ƒç›´æ¥èª¿ç”¨ LLM
        # ä¸¦å°‡ç›¸é—œæ–‡æª”ä½œç‚ºä¸Šä¸‹æ–‡å‚³å…¥
        
        # æ ¼å¼åŒ–ç›¸é—œæ–‡æª”ä½œç‚ºä¸Šä¸‹æ–‡
        context_docs = [doc for doc, score in relevant_docs_with_scores]
        formatted_context = "\n\n".join(doc.page_content for doc in context_docs)
        
        # é‡æ–°æ§‹å»ºä¸€å€‹è‡¨æ™‚çš„ chain ä¾†è™•ç†é€™å€‹å•é¡Œï¼Œæˆ–è€…ç›´æ¥èª¿ç”¨ LLM
        # é€™è£¡ç‚ºäº†ç°¡åŒ–ï¼Œç›´æ¥ä½¿ç”¨ LLM å’Œä¸€å€‹ç°¡å–®çš„ prompt
        model = _global_state.get('model')
        if not model:
            raise Exception("èªè¨€æ¨¡å‹æœªåˆå§‹åŒ–")
            
        prompt = ChatPromptTemplate.from_template(
            """æ ¹æ“šä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚è«‹æä¾›è©³ç´°ã€æº–ç¢ºçš„ç­”æ¡ˆã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºã€‚

ä¸Šä¸‹æ–‡: {context}

å•é¡Œ: {question}

å›ç­”:"""
        )
        
        chain = prompt | model | StrOutputParser()
        raw_answer = chain.invoke({"context": formatted_context, "question": question})
        
        print("ğŸ’¡ ç­”æ¡ˆ:")
        print("-" * 50)
        print(raw_answer)
        print("-" * 50)
        
        return raw_answer, relevant_docs_with_scores
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        return "", []

# çµ±è¨ˆå’Œå…ƒæ•¸æ“šå‡½æ•¸

def get_vectorstore_stats() -> Dict:
    """ç²å–å‘é‡å­˜å„²çµ±è¨ˆä¿¡æ¯"""
    qdrant_client = _global_state.get('qdrant_client')
    vectorstore = _global_state.get('vectorstore')
    
    if not qdrant_client or not vectorstore:
        return {}
    
    try:
        collection_name = vectorstore.collection_name
        collection_info = qdrant_client.get_collection(collection_name)
        
        return {
            "collection_name": collection_name,
            "total_vectors": collection_info.points_count,
            "vector_size": collection_info.config.params.vectors.size,
            "distance_metric": collection_info.config.params.vectors.distance
        }
    except Exception as e:
        print(f"âš ï¸ ç²å–çµ±è¨ˆä¿¡æ¯å¤±æ•—: {e}")
        return {}


def get_qdrant_file_metadata(collection_name: str) -> Dict[str, str]:
    """å¾ Qdrant ç²å–å·²å­˜å„²æ–‡ä»¶çš„å…ƒæ•¸æ“š
    
    Args:
        collection_name: é›†åˆåç¨±
        
    Returns:
        Dict[str, str]: æ–‡ä»¶è·¯å¾‘åˆ°å“ˆå¸Œå€¼çš„æ˜ å°„
    """
    qdrant_client = _global_state.get('qdrant_client')
    if not qdrant_client:
        return {}
    
    try:
        # æŸ¥è©¢ Qdrant ä¸­çš„æ‰€æœ‰é»
        points, next_page_offset = qdrant_client.scroll(
            collection_name=collection_name,
            scroll_filter=None,
            limit=10000,
            with_payload=True,
            with_vectors=False
        )
        
        # æå–æ–‡ä»¶å…ƒæ•¸æ“š
        file_metadata = {}
        processed_count = 0
        
        for point in points:
            processed_count += 1
            
            # æª¢æŸ¥ payload æ˜¯å¦å­˜åœ¨
            if not point.payload:
                continue
            
            # æª¢æŸ¥ LangChain æ ¼å¼ï¼ˆmetadata åœ¨ payload.metadata ä¸­ï¼‰
            if 'metadata' in point.payload and point.payload['metadata'] is not None:
                metadata = point.payload['metadata']
                if isinstance(metadata, dict) and 'source' in metadata and 'file_hash' in metadata:
                    file_path = metadata['source']
                    file_hash = metadata['file_hash']
                    if file_path and file_hash:  # ç¢ºä¿ä¸æ˜¯ç©ºå€¼
                        file_metadata[file_path] = file_hash
            
            # æª¢æŸ¥ç›´æ¥æ ¼å¼ï¼ˆmetadata ç›´æ¥åœ¨ payload ä¸­ï¼‰
            elif 'source' in point.payload and 'file_hash' in point.payload:
                file_path = point.payload['source']
                file_hash = point.payload['file_hash']
                if file_path and file_hash:  # ç¢ºä¿ä¸æ˜¯ç©ºå€¼
                    file_metadata[file_path] = file_hash
        
        print(f"ğŸ“‹ å¾ Qdrant è™•ç†äº† {processed_count} å€‹é»ï¼Œæ‰¾åˆ° {len(file_metadata)} å€‹æ–‡ä»¶è¨˜éŒ„")
        
        # èª¿è©¦ï¼šé¡¯ç¤ºæ‰¾åˆ°çš„æ–‡ä»¶
        if file_metadata:
            print("   æ‰¾åˆ°çš„æ–‡ä»¶:")
            for file_path, file_hash in file_metadata.items():
                print(f"   - {file_path}: {file_hash[:8]}...")
        
        return file_metadata
        
    except Exception as e:
        print(f"âš ï¸ ç²å– Qdrant å…ƒæ•¸æ“šå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return {}


def calculate_file_hash(file_path: str) -> str:
    """è¨ˆç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        
    Returns:
        str: MD5 å“ˆå¸Œå€¼
    """
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è¨ˆç®—æ–‡ä»¶å“ˆå¸Œ: {file_path}, éŒ¯èª¤: {e}")
        return ""

# é«˜ç´šä¾¿æ·å‡½æ•¸ï¼ˆä¿æŒèˆ‡åŸé¡åˆ¥çš„ç›¸å®¹æ€§ï¼‰

def create_raptor_core(config: Dict = None, 
                      openai_api_key: str = None,
                      openai_api_keys: List[str] = None) -> bool:
    """å‰µå»ºä¸¦åˆå§‹åŒ– RAPTOR æ ¸å¿ƒ
    
    Args:
        config: é…ç½®å­—å…¸
        openai_api_key: å–®å€‹ API Key
        openai_api_keys: å¤šå€‹ API Key åˆ—è¡¨
        
    Returns:
        bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
    """
    # åˆå§‹åŒ–æ ¸å¿ƒ
    init_raptor_core(config)
    
    # è¨­ç½®æ¨¡å‹
    if openai_api_keys or openai_api_key:
        success = setup_models(
            openai_api_key=openai_api_key,
            openai_api_keys=openai_api_keys
        )
        if not success:
            print("âŒ æ¨¡å‹è¨­ç½®å¤±æ•—")
            return False
    
    return True


def get_current_state() -> Dict:
    """ç²å–ç•¶å‰å…¨åŸŸç‹€æ…‹ï¼ˆç”¨æ–¼é™¤éŒ¯ï¼‰"""
    return {
        'config_loaded': _global_state['config'] is not None,
        'embd_loaded': _global_state['embd'] is not None,
        'model_loaded': _global_state['model'] is not None,
        'qdrant_connected': _global_state['qdrant_client'] is not None,
        'vectorstore_ready': _global_state['vectorstore'] is not None,
        'rag_chain_built': _global_state['rag_chain'] is not None,
        'api_keys_count': len(_global_state['api_keys']),
        'current_key_index': _global_state['current_key_index']
    }


def reset_raptor_core():
    """é‡ç½® RAPTOR æ ¸å¿ƒç‹€æ…‹"""
    global _global_state
    _global_state = {
        'config': None,
        'embd': None,
        'model': None,
        'qdrant_client': None,
        'vectorstore': None,
        'rag_chain': None,
        'api_keys': [],
        'current_key_index': 0
    }
    print("ğŸ”„ RAPTOR Core ç‹€æ…‹å·²é‡ç½®")

# å®Œæ•´æµç¨‹ä¾¿æ·å‡½æ•¸

def full_setup_raptor_system(config: Dict = None,
                             qdrant_url: str = None,
                             qdrant_api_key: str = None,
                             collection_name: str = "rag_knowledge") -> bool:
    """å®Œæ•´è¨­ç½® RAPTOR ç³»çµ±
    
    Args:
        config: é…ç½®å­—å…¸
        qdrant_url: Qdrant URLï¼ˆå¦‚æœç‚º None å‰‡å¾ç’°å¢ƒè®Šæ•¸å–å¾—ï¼‰
        qdrant_api_key: Qdrant API Keyï¼ˆå¦‚æœç‚º None å‰‡å¾ç’°å¢ƒè®Šæ•¸å–å¾—ï¼‰
        collection_name: é›†åˆåç¨±
        
    Returns:
        bool: è¨­ç½®æ˜¯å¦æˆåŠŸ
    """
    print("ğŸš€ é–‹å§‹å®Œæ•´è¨­ç½® RAPTOR ç³»çµ±...")
    
    # 1. è¼‰å…¥ API Keys
    api_keys = load_api_keys_from_files()
    if not api_keys:
        print("âŒ æ‰¾ä¸åˆ°ä»»ä½• OpenAI API Keyï¼")
        return False
    
    # 2. åˆå§‹åŒ–æ ¸å¿ƒ
    if not create_raptor_core(config, openai_api_keys=api_keys):
        return False
    
    # 3. è¨­ç½® Qdrant
    if not qdrant_url:
        qdrant_url = os.getenv("QDRANT_URL")
    if not qdrant_api_key:
        qdrant_api_key = os.getenv("QDRANT_API_KEY")
    
    if not qdrant_url or not qdrant_api_key:
        print("âŒ ç¼ºå°‘ Qdrant é…ç½®")
        return False
    
    if not setup_qdrant(qdrant_url, qdrant_api_key, collection_name):
        return False
    
    # 4. å»ºç«‹ RAG éˆ
    if not build_rag_chain():
        return False
    
    print("âœ… RAPTOR ç³»çµ±å®Œæ•´è¨­ç½®æˆåŠŸï¼")
    return True


def quick_process_and_ask(directory_path: str = "knowledge_docs",
                         question: str = "é€™äº›æ–‡æª”çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ",
                         show_sources: bool = True) -> str:
    """å¿«é€Ÿè™•ç†æ–‡æª”ä¸¦å›ç­”å•é¡Œ
    
    Args:
        directory_path: æ–‡æª”ç›®éŒ„
        question: å•é¡Œ
        show_sources: æ˜¯å¦é¡¯ç¤ºä¾†æº
        
    Returns:
        str: å›ç­”
    """
    # è¼‰å…¥ä¸¦è™•ç†æ–‡æª”
    documents = load_documents_from_directory(directory_path)
    if not documents:
        return "âŒ æ‰¾ä¸åˆ°ä»»ä½•æ–‡æª”"
    
    # ä½¿ç”¨ RAPTOR è™•ç†
    all_texts = process_documents_with_raptor(documents)
    if not all_texts:
        return "âŒ æ–‡æª”è™•ç†å¤±æ•—"
    
    # æ·»åŠ åˆ°å‘é‡å­˜å„²
    if not add_texts_to_vectorstore(all_texts):
        return "âŒ å‘é‡å­˜å„²æ·»åŠ å¤±æ•—"
    
    # é‡å»º RAG éˆ
    if not build_rag_chain():
        return "âŒ RAG éˆå»ºç«‹å¤±æ•—"
    
    # å›ç­”å•é¡Œ
    return ask_question(question, show_sources)


# ä½¿ç”¨ç¯„ä¾‹

def example_usage():
    """ä½¿ç”¨ç¯„ä¾‹"""
    print("ğŸ¯ RAPTOR Core å‡½æ•¸å¼ç‰ˆæœ¬ä½¿ç”¨ç¯„ä¾‹")
    print("=" * 50)
    
    # 1. å®Œæ•´è¨­ç½®ç³»çµ±
    config = {
        "chunk_size": 1000,
        "chunk_overlap": 100,
        "n_levels": 2,
        "embedding_model": "text-embedding-3-small",
        "llm_model": "gpt-4o-mini",
        "retrieval_k": 5
    }
    
    success = full_setup_raptor_system(config)
    if not success:
        print("âŒ ç³»çµ±è¨­ç½®å¤±æ•—")
        return
    
    # 2. è™•ç†æ–‡æª”ä¸¦å›ç­”å•é¡Œ
    answer = quick_process_and_ask(
        directory_path="knowledge_docs",
        question="é€™äº›æ–‡æª”çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ",
        show_sources=True
    )
    
    print(f"å›ç­”: {answer}")
    
    # 3. é¡¯ç¤ºç³»çµ±ç‹€æ…‹
    state = get_current_state()
    print(f"\nç³»çµ±ç‹€æ…‹: {state}")


if __name__ == "__main__":
    example_usage()