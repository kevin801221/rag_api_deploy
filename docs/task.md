# RAG å‘é‡è³‡æ–™åº«å®Œæ•´æ‡‰ç”¨æŒ‡å—

## ä¸€ã€RAG å‘é‡è³‡æ–™åº«å»ºç½®æµç¨‹

### 1.1 Qdrant å‘é‡è³‡æ–™åº«å»ºç½®

**Qdrant Cloud å»ºç½®æ­¥é©Ÿï¼š**

1. **å»ºæ§‹ Cluster**
   - å‰å¾€ [Qdrant Cloud](https://cloud.qdrant.io)
   - è¨»å†Šå¸³è™Ÿä¸¦å»ºç«‹æ–°çš„ cluster

2. **å»ºç«‹ API Key**
   - åœ¨ cluster ç®¡ç†é é¢ç”¢ç”Ÿ API key
   - è¨˜éŒ„ç«¯é»å’Œ API key è³‡è¨Š

3. **é€£ç·šè³‡è¨Šç¯„ä¾‹**
   ```python
   # é…ç½®è³‡è¨Š
   QDRANT_URL = ""
   QDRANT_API_KEY = ""
   MONGODB_URI = ""
   # é€£ç·šè¨­å®š
   from qdrant_client import QdrantClient
   
   client = QdrantClient(
       url=QDRANT_URL,
       api_key=QDRANT_API_KEY,
   )
   ```

**ğŸ’¡ è£œå……è³‡è¨Šï¼š**
- å…è²»ç‰ˆæœ¬æä¾› 1GB å„²å­˜ç©ºé–“
- æ”¯æ´ 1536 ç¶­å‘é‡ï¼ˆé©ç”¨ OpenAI embeddingsï¼‰
- å»ºè­°åœ¨ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨ HTTPS é€£ç·š

### 1.2 å…¶ä»–å‘é‡è³‡æ–™åº«é¸é …

| è³‡æ–™åº« | ç‰¹è‰² | é©ç”¨å ´æ™¯ |
|--------|------|----------|
| **Weaviate** | å¼·å‹åˆ¥ schemaï¼Œæ”¯æ´å¤šæ¨¡æ…‹ | ä¼æ¥­ç´šæ‡‰ç”¨ |
| **Pinecone** | å…¨è¨—ç®¡ï¼Œé«˜æ•ˆèƒ½ | å¿«é€Ÿéƒ¨ç½² |
| **Chroma** | è¼•é‡ç´šï¼Œæ˜“æ–¼æœ¬åœ°é–‹ç™¼ | åŸå‹é–‹ç™¼ |
| **Milvus** | é–‹æºï¼Œå¯è‡ªéƒ¨ç½² | è‡ªå»ºåŸºç¤è¨­æ–½ |

## äºŒã€æ–‡ä»¶è½‰å‘é‡çš„å®Œæ•´æ­¥é©Ÿ

### 2.1 æ–‡ä»¶è¼‰å…¥ (Document Loading)

**æ”¯æ´æ ¼å¼èˆ‡å°æ‡‰ Loaderï¼š**

```python
from langchain_community.document_loaders import (
    TextLoader,      # .txt
    PyPDFLoader,     # .pdf
    CSVLoader,       # .csv
    JSONLoader,      # .json
    UnstructuredXMLLoader,  # .xml
    UnstructuredHTMLLoader, # .html
    Docx2txtLoader,  # .docx
    UnstructuredPowerPointLoader,  # .ppt, .pptx
)

# è¼‰å…¥ç¯„ä¾‹
loader = PyPDFLoader("document.pdf")
documents = loader.load()
```

**ğŸ“š åƒè€ƒè³‡æºï¼š** [LangChain Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)

### 2.2 æ–‡ä»¶åˆ‡æ®µ (Text Splitting)

**å››ç¨®ä¸»è¦åˆ‡æ®µç­–ç•¥ï¼š**

#### 1. å­—å…ƒåˆ‡æ®µ (CharacterTextSplitter)
```python
from langchain.text_splitter import CharacterTextSplitter

splitter = CharacterTextSplitter(
    separator="\n\n",
    chunk_size=1000,
    chunk_overlap=200
)
```

#### 2. Token åˆ‡æ®µ (TokenTextSplitter)
```python
from langchain.text_splitter import TokenTextSplitter

splitter = TokenTextSplitter(
    encoding_name="cl100k_base",
    chunk_size=512,
    chunk_overlap=50
)
```

#### 3. éæ­¸å­—å…ƒåˆ‡æ®µ (RecursiveCharacterTextSplitter)
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", " ", ""],
    chunk_size=1000,
    chunk_overlap=100
)
```

#### 4. éæ­¸ Token åˆ‡æ®µ (RecursiveTokenTextSplitter)
```python
from langchain.text_splitter import RecursiveTokenTextSplitter

splitter = RecursiveTokenTextSplitter(
    encoding_name="cl100k_base",
    chunk_size=512,
    chunk_overlap=50
)
```

**ğŸ¯ é¸æ“‡å»ºè­°ï¼š**
- **ä¸€èˆ¬æ–‡æª”**ï¼šRecursiveCharacterTextSplitter
- **ç¨‹å¼ç¢¼**ï¼šèªè¨€ç‰¹å®šçš„ RecursiveCharacterTextSplitter
- **çµæ§‹åŒ–è³‡æ–™**ï¼šTokenTextSplitter
- **å¤§å‹æ–‡æª”**ï¼šRecursiveTokenTextSplitter

### 2.3 å‘é‡åµŒå…¥ (Embedding)

**æ”¯æ´çš„åµŒå…¥æ¨¡å‹ï¼š**

```python
# OpenAI Embeddings
from langchain_openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Cohere Embeddings
from langchain_cohere import CohereEmbeddings
embeddings = CohereEmbeddings(model="embed-multilingual-v2.0")

# HuggingFace Embeddings
from langchain_huggingface import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Sentence Transformers
from langchain_community.embeddings import SentenceTransformerEmbeddings
embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
```

**ğŸ“Š æ¨¡å‹æ¯”è¼ƒï¼š**

| æ¨¡å‹ | ç¶­åº¦ | èªè¨€æ”¯æ´ | æ•ˆèƒ½ | æˆæœ¬ |
|------|------|----------|------|------|
| text-embedding-3-small | 1536 | å¤šèªè¨€ | é«˜ | ä¸­ |
| text-embedding-3-large | 3072 | å¤šèªè¨€ | æœ€é«˜ | é«˜ |
| all-MiniLM-L6-v2 | 384 | è‹±æ–‡ç‚ºä¸» | ä¸­ | å…è²» |

**ğŸ“š åƒè€ƒè³‡æºï¼š** [LangChain Text Embeddings](https://python.langchain.com/docs/integrations/text_embedding/)

### 2.4 å‘é‡å„²å­˜ (Vector Storage)

```python
# Qdrant
from langchain_qdrant import Qdrant
vector_store = Qdrant.from_documents(
    documents, embeddings, url=QDRANT_URL, api_key=QDRANT_API_KEY
)

# Weaviate
from langchain_weaviate import WeaviateVectorStore
vector_store = WeaviateVectorStore.from_documents(documents, embeddings)
```

**ğŸ“š åƒè€ƒè³‡æºï¼š** [LangChain VectorStores](https://python.langchain.com/docs/concepts/vectorstores/)

## ä¸‰ã€åˆ‡æ®µèˆ‡ Metadata æœ€ä½³ç­–ç•¥

### 3.1 ç’°å¢ƒæº–å‚™

**å¿…è¦å¥—ä»¶å®‰è£ï¼š**
```bash
pip install -qU langchain langchain-openai langchain-mongodb langchain-experimental ragas pymongo tqdm
```

**é€£ç·šè¨­å®šï¼š**
```python
import getpass
import os
from openai import OpenAI

# MongoDB é€£ç·š
MONGODB_URI = getpass.getpass("Enter your MongoDB connection string:")

# OpenAI é€£ç·š
os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API Key:")
openai_client = OpenAI()
```

### 3.2 è³‡æ–™è¼‰å…¥èˆ‡æª¢è¦–

```python
from langchain_community.document_loaders import WebBaseLoader

# è¼‰å…¥æ¸¬è©¦è³‡æ–™
web_loader = WebBaseLoader([
    "https://peps.python.org/pep-0483/",
    "https://peps.python.org/pep-0008/",
    "https://peps.python.org/pep-0257/",
])
pages = web_loader.load()

# Document çµæ§‹ç¯„ä¾‹
print("Document çµæ§‹ï¼š")
print(f"page_content: {pages[0].page_content[:100]}...")
print(f"metadata: {pages[0].metadata}")
```

**ğŸ“‹ Document çµæ§‹èªªæ˜ï¼š**
- `page_content`ï¼šæ–‡æª”å…§å®¹
- `metadata`ï¼šåŒ…å« sourceã€titleã€descriptionã€language ç­‰è³‡è¨Š

### 3.3 åˆ‡æ®µç­–ç•¥å‡½æ•¸å®šç¾©

#### å›ºå®š Token åˆ‡æ®µ
```python
from langchain.text_splitter import TokenTextSplitter
from typing import List, Optional

def fixed_token_split(docs: List[Document], chunk_size: int, chunk_overlap: int) -> List[Document]:
    """
    å›ºå®š token åˆ‡æ®µ
    
    Args:
        docs: æ–‡æª”åˆ—è¡¨
        chunk_size: åˆ‡æ®µå¤§å°ï¼ˆtoken æ•¸é‡ï¼‰
        chunk_overlap: é‡ç–Š token æ•¸é‡
    
    Returns:
        åˆ‡æ®µå¾Œçš„æ–‡æª”åˆ—è¡¨
    """
    splitter = TokenTextSplitter(
        encoding_name="cl100k_base", 
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap
    )
    return splitter.split_documents(docs)
```

#### éæ­¸åˆ‡æ®µ
```python
from langchain.text_splitter import Language, RecursiveCharacterTextSplitter

def recursive_split(
    docs: List[Document],
    chunk_size: int,
    chunk_overlap: int,
    language: Optional[Language] = None,
) -> List[Document]:
    """
    éæ­¸åˆ‡æ®µ
    
    Args:
        docs: æ–‡æª”åˆ—è¡¨
        chunk_size: åˆ‡æ®µå¤§å°ï¼ˆtoken æ•¸é‡ï¼‰
        chunk_overlap: é‡ç–Š token æ•¸é‡
        language: ç¨‹å¼èªè¨€é¡å‹ï¼ˆå¯é¸ï¼‰
    
    Returns:
        åˆ‡æ®µå¾Œçš„æ–‡æª”åˆ—è¡¨
    """
    separators = ["\n\n", "\n", " ", ""]
    
    if language is not None:
        try:
            separators = RecursiveCharacterTextSplitter.get_separators_for_language(language)
        except (NameError, ValueError) as e:
            print(f"èªè¨€ {language} ç„¡å¯ç”¨åˆ†éš”ç¬¦ï¼Œä½¿ç”¨é è¨­å€¼")
    
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        encoding_name="cl100k_base",
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=separators,
    )
    return splitter.split_documents(docs)
```

#### èªç¾©åˆ‡æ®µ
```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

def semantic_split(docs: List[Document]) -> List[Document]:
    """
    èªç¾©åˆ‡æ®µ
    
    Args:
        docs: æ–‡æª”åˆ—è¡¨
    
    Returns:
        èªç¾©åˆ‡æ®µå¾Œçš„æ–‡æª”åˆ—è¡¨
    """
    splitter = SemanticChunker(
        OpenAIEmbeddings(), 
        breakpoint_threshold_type="percentile"
    )
    return splitter.split_documents(docs)
```

**ğŸ”§ èªç¾©åˆ‡æ®µåƒæ•¸é¸é …ï¼š**
- `percentile`ï¼šä½¿ç”¨ 95% åˆ†ä½æ•¸ä½œç‚ºé–¾å€¼
- `standard_deviation`ï¼šä½¿ç”¨æ¨™æº–å·®æ–¹æ³•
- `interquartile`ï¼šä½¿ç”¨å››åˆ†ä½è·æ–¹æ³•

### 3.4 è©•ä¼°è³‡æ–™é›†ç”Ÿæˆ

```python
from ragas import RunConfig
from ragas.testset.generator import TestsetGenerator
from ragas.testset.evolutions import simple, reasoning, multi_context
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# è¨­å®šåŸ·è¡Œé…ç½®
RUN_CONFIG = RunConfig(max_workers=4, max_wait=180)

# é…ç½®ç”Ÿæˆæ¨¡å‹
generator_llm = ChatOpenAI(model="gpt-3.5-turbo-16k")
critic_llm = ChatOpenAI(model="gpt-4")
embeddings = OpenAIEmbeddings()

generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, embeddings)

# è¨­å®šå•é¡Œé¡å‹åˆ†ä½ˆ
distributions = {
    simple: 0.5,        # ç°¡å–®å•é¡Œ 50%
    multi_context: 0.4, # å¤šèªå¢ƒå•é¡Œ 40%
    reasoning: 0.1      # æ¨ç†å•é¡Œ 10%
}

# ç”Ÿæˆæ¸¬è©¦è³‡æ–™é›†
testset = generator.generate_with_langchain_docs(
    pages, 10, distributions, run_config=RUN_CONFIG
)
```

**ğŸ“Š å•é¡Œé¡å‹èªªæ˜ï¼š**
- **simple**ï¼šç›´æ¥å¾æ–‡æª”ä¸­å¯ä»¥æ‰¾åˆ°ç­”æ¡ˆçš„å•é¡Œ
- **multi_context**ï¼šéœ€è¦å¤šå€‹æ–‡æª”ç‰‡æ®µæ‰èƒ½å›ç­”çš„å•é¡Œ
- **reasoning**ï¼šéœ€è¦æ¨ç†å’Œåˆ†æçš„è¤‡é›œå•é¡Œ

### 3.5 MongoDB Atlas å‘é‡å„²å­˜è¨­å®š

```python
from langchain_mongodb import MongoDBAtlasVectorSearch
from pymongo import MongoClient

# MongoDB é€£ç·šè¨­å®š
client = MongoClient(MONGODB_URI)
DB_NAME = "evals"
COLLECTION_NAME = "chunking"
ATLAS_VECTOR_SEARCH_INDEX_NAME = "vector_index"
MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]

def create_vector_store(docs: List[Document]) -> MongoDBAtlasVectorSearch:
    """
    å»ºç«‹ MongoDB Atlas å‘é‡å„²å­˜
    
    Args:
        docs: æ–‡æª”åˆ—è¡¨
    
    Returns:
        MongoDB Atlas å‘é‡å„²å­˜å¯¦ä¾‹
    """
    vector_store = MongoDBAtlasVectorSearch.from_documents(
        documents=docs,
        embedding=OpenAIEmbeddings(model="text-embedding-3-small"),
        collection=MONGODB_COLLECTION,
        index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,
    )
    return vector_store
```

**âš™ï¸ MongoDB Atlas ç´¢å¼•é…ç½®ï¼š**
```json
{
  "fields": [
    {
      "numDimensions": 1536,
      "path": "embedding",
      "similarity": "cosine",
      "type": "vector"
    }
  ]
}
```

### 3.6 åˆ‡æ®µç­–ç•¥è©•ä¼°

```python
from tqdm import tqdm
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import context_precision, context_recall
import nest_asyncio

# è¨­å®š asyncio
nest_asyncio.apply()
tqdm.get_lock().locks = []

# æº–å‚™è©•ä¼°è³‡æ–™
QUESTIONS = testset.question.to_list()
GROUND_TRUTH = testset.ground_truth.to_list()

def perform_eval(docs: List[Document]) -> Dict[str, float]:
    """
    åŸ·è¡Œ RAGAS è©•ä¼°
    
    Args:
        docs: åˆ‡æ®µå¾Œçš„æ–‡æª”åˆ—è¡¨
    
    Returns:
        è©•ä¼°æŒ‡æ¨™å­—å…¸
    """
    eval_data = {
        "question": QUESTIONS,
        "ground_truth": GROUND_TRUTH,
        "contexts": [],
    }
    
    # æ¸…ç©ºç¾æœ‰æ–‡æª”
    print(f"æ¸…ç©ºé›†åˆ {DB_NAME}.{COLLECTION_NAME} ä¸­çš„ç¾æœ‰æ–‡æª”")
    MONGODB_COLLECTION.delete_many({})
    print("æ¸…ç©ºå®Œæˆ")
    
    # å»ºç«‹å‘é‡å„²å­˜
    vector_store = create_vector_store(docs)
    
    # ç‚ºè©•ä¼°è³‡æ–™é›†ç²å–ç›¸é—œæ–‡æª”
    print("ç²å–è©•ä¼°é›†çš„èªå¢ƒ")
    for question in tqdm(QUESTIONS):
        retrieved_docs = vector_store.similarity_search(question, k=3)
        eval_data["contexts"].append([doc.page_content for doc in retrieved_docs])
    
    # RAGAS è©•ä¼°
    dataset = Dataset.from_dict(eval_data)
    print("åŸ·è¡Œè©•ä¼°")
    
    result = evaluate(
        dataset=dataset,
        metrics=[context_precision, context_recall],
        run_config=RUN_CONFIG,
        raise_exceptions=False,
    )
    return result
```

### 3.7 æ‰¹é‡è©•ä¼°åŸ·è¡Œ

```python
# è©•ä¼°ä¸åŒåˆ‡æ®µç­–ç•¥
strategies_results = {}

for chunk_size in [100, 200, 500, 1000]:
    chunk_overlap = int(0.15 * chunk_size)  # 15% é‡ç–Šç‡
    
    print(f"\n======= åˆ‡æ®µå¤§å°: {chunk_size} =======")
    
    # 1. å›ºå®š token ç„¡é‡ç–Š
    print("------ å›ºå®š token ç„¡é‡ç–Š ------")
    result = perform_eval(fixed_token_split(pages, chunk_size, 0))
    print(f"çµæœ: {result}")
    
    # 2. å›ºå®š token æœ‰é‡ç–Š
    print("------ å›ºå®š token æœ‰é‡ç–Š ------")
    result = perform_eval(fixed_token_split(pages, chunk_size, chunk_overlap))
    print(f"çµæœ: {result}")
    
    # 3. éæ­¸åˆ‡æ®µæœ‰é‡ç–Š
    print("------ éæ­¸åˆ‡æ®µæœ‰é‡ç–Š ------")
    result = perform_eval(recursive_split(pages, chunk_size, chunk_overlap))
    print(f"çµæœ: {result}")
    
    # 4. Python ç‰¹å®šéæ­¸åˆ‡æ®µ
    print("------ Python ç‰¹å®šéæ­¸åˆ‡æ®µ ------")
    result = perform_eval(recursive_split(pages, chunk_size, chunk_overlap, Language.PYTHON))
    print(f"çµæœ: {result}")

# 5. èªç¾©åˆ‡æ®µ
print("\n------ èªç¾©åˆ‡æ®µ ------")
result = perform_eval(semantic_split(pages))
print(f"çµæœ: {result}")
```

## å››ã€è©•ä¼°æŒ‡æ¨™èˆ‡çµæœåˆ†æ

### 4.1 è©•ä¼°æŒ‡æ¨™èªªæ˜

**Context Precisionï¼ˆèªå¢ƒç²¾ç¢ºåº¦ï¼‰**
- **å®šç¾©**ï¼šè©•ä¼°æª¢ç´¢å™¨æŒ‰ç›¸é—œæ€§é †åºæ’åˆ—æª¢ç´¢é …ç›®çš„èƒ½åŠ›
- **è¨ˆç®—**ï¼šç›¸é—œæª¢ç´¢é …ç›® / ç¸½æª¢ç´¢é …ç›®
- **ç›®æ¨™**ï¼šè¶Šæ¥è¿‘ 1.0 è¶Šå¥½

**Context Recallï¼ˆèªå¢ƒå¬å›ç‡ï¼‰**
- **å®šç¾©**ï¼šè¡¡é‡æª¢ç´¢èªå¢ƒèˆ‡çœŸå¯¦ç­”æ¡ˆçš„å°é½Šç¨‹åº¦
- **è¨ˆç®—**ï¼šæª¢ç´¢åˆ°çš„ç›¸é—œè³‡è¨Š / æ‰€æœ‰ç›¸é—œè³‡è¨Š
- **ç›®æ¨™**ï¼šè¶Šæ¥è¿‘ 1.0 è¶Šå¥½

### 4.2 çµæœè§£è®€ç¯„ä¾‹

```python
# ç¯„ä¾‹è©•ä¼°çµæœ
results_example = {
    "ç­–ç•¥": ["å›ºå®šTokenç„¡é‡ç–Š", "å›ºå®šTokenæœ‰é‡ç–Š", "éæ­¸åˆ‡æ®µ", "Pythonç‰¹å®š", "èªç¾©åˆ‡æ®µ"],
    "æœ€ä½³åˆ‡æ®µå¤§å°": [500, 100, 100, 100, "N/A"],
    "Context Precision": [0.8833, 0.9, 0.9, 0.9833, 0.9],
    "Context Recall": [0.95, 0.95, 0.9833, 0.9833, 0.8187]
}

# åœ¨æ­¤ç¯„ä¾‹ä¸­ï¼ŒPythonç‰¹å®šåˆ†å‰²åœ¨å…©å€‹æŒ‡æ¨™ä¸Šéƒ½è¡¨ç¾æœ€ä½³
```

### 4.3 æœ€ä½³åŒ–å»ºè­°

**åŸºæ–¼è©•ä¼°çµæœçš„ç­–ç•¥é¸æ“‡ï¼š**

1. **é«˜ç²¾ç¢ºåº¦éœ€æ±‚**ï¼šé¸æ“‡ Context Precision æœ€é«˜çš„ç­–ç•¥
2. **é«˜å¬å›ç‡éœ€æ±‚**ï¼šé¸æ“‡ Context Recall æœ€é«˜çš„ç­–ç•¥
3. **å¹³è¡¡è€ƒé‡**ï¼šè¨ˆç®— F1 åˆ†æ•¸ = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

**åƒæ•¸èª¿å„ªå»ºè­°ï¼š**
```python
def optimize_parameters():
    """åƒæ•¸æœ€ä½³åŒ–ç¯„ä¾‹"""
    best_f1 = 0
    best_config = {}
    
    for size in [100, 200, 300, 500]:
        for overlap_ratio in [0.1, 0.15, 0.2]:
            overlap = int(size * overlap_ratio)
            
            # åŸ·è¡Œè©•ä¼°
            chunks = recursive_split(pages, size, overlap, Language.PYTHON)
            results = perform_eval(chunks)
            
            # è¨ˆç®— F1 åˆ†æ•¸
            precision = results['context_precision']
            recall = results['context_recall']
            f1 = 2 * (precision * recall) / (precision + recall)
            
            if f1 > best_f1:
                best_f1 = f1
                best_config = {
                    'chunk_size': size,
                    'overlap': overlap,
                    'f1_score': f1
                }
    
    return best_config
```

## äº”ã€é€²éš Metadata è¨­è¨ˆç­–ç•¥

### 5.1 åŸºç¤ Metadata çµæ§‹

```python
def enhance_metadata(document, chunk_index, total_chunks):
    """å¢å¼· metadata è³‡è¨Š"""
    enhanced_metadata = {
        # åŸå§‹ metadata
        **document.metadata,
        
        # åˆ‡æ®µè³‡è¨Š
        "chunk_id": f"{document.metadata.get('source', 'unknown')}_{chunk_index}",
        "chunk_index": chunk_index,
        "total_chunks": total_chunks,
        "chunk_size": len(document.page_content),
        
        # å…§å®¹åˆ†æ
        "has_code": "```" in document.page_content or "def " in document.page_content,
        "has_table": "|" in document.page_content,
        "paragraph_count": document.page_content.count("\n\n"),
        
        # æ™‚é–“æˆ³è¨˜
        "indexed_at": datetime.now().isoformat(),
        
        # å…§å®¹æ‘˜è¦ï¼ˆå¯é¸ï¼‰
        "summary": generate_summary(document.page_content) if len(document.page_content) > 1000 else None
    }
    
    return enhanced_metadata
```

### 5.2 å‹•æ…‹ Metadata ç”Ÿæˆ

```python
def generate_dynamic_metadata(text):
    """å‹•æ…‹ç”Ÿæˆ metadata"""
    import re
    from collections import Counter
    
    metadata = {
        # åŸºæœ¬çµ±è¨ˆ
        "word_count": len(text.split()),
        "char_count": len(text),
        "sentence_count": len(re.split(r'[.!?]+', text)),
        
        # å…§å®¹ç‰¹å¾µ
        "has_numbers": bool(re.search(r'\d+', text)),
        "has_urls": bool(re.search(r'https?://', text)),
        "has_emails": bool(re.search(r'\S+@\S+', text)),
        
        # èªè¨€æª¢æ¸¬
        "language": detect_language(text),
        
        # ä¸»é¡Œé—œéµè©
        "keywords": extract_keywords(text, top_k=5),
        
        # é›£åº¦è©•ä¼°
        "reading_difficulty": assess_reading_difficulty(text)
    }
    
    return metadata
```

## å…­ã€ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²å»ºè­°

### 6.1 æ•ˆèƒ½ç›£æ§

```python
import time
from functools import wraps

def monitor_performance(func):
    """æ•ˆèƒ½ç›£æ§è£é£¾å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        print(f"{func.__name__} åŸ·è¡Œæ™‚é–“: {end_time - start_time:.2f} ç§’")
        return result
    return wrapper

@monitor_performance
def chunking_pipeline(documents):
    """å®Œæ•´åˆ‡æ®µæµæ°´ç·š"""
    # åŸ·è¡Œåˆ‡æ®µé‚è¼¯
    pass
```

### 6.2 éŒ¯èª¤è™•ç†

```python
def robust_chunking(documents, strategy="recursive", **kwargs):
    """ç©©å¥çš„åˆ‡æ®µè™•ç†"""
    try:
        if strategy == "recursive":
            return recursive_split(documents, **kwargs)
        elif strategy == "semantic":
            return semantic_split(documents)
        elif strategy == "fixed":
            return fixed_token_split(documents, **kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ç­–ç•¥: {strategy}")
            
    except Exception as e:
        print(f"åˆ‡æ®µå¤±æ•—: {e}")
        # å›é€€åˆ°æœ€ç°¡å–®çš„ç­–ç•¥
        return fixed_token_split(documents, chunk_size=500, chunk_overlap=50)
```

### 6.3 å¿«å–æ©Ÿåˆ¶

```python
import hashlib
import pickle
import os

class ChunkingCache:
    """åˆ‡æ®µçµæœå¿«å–"""
    
    def __init__(self, cache_dir="./cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def _get_cache_key(self, documents, strategy, **kwargs):
        """ç”Ÿæˆå¿«å–éµå€¼"""
        content_hash = hashlib.md5(
            "".join([doc.page_content for doc in documents]).encode()
        ).hexdigest()
        
        params_hash = hashlib.md5(
            f"{strategy}_{kwargs}".encode()
        ).hexdigest()
        
        return f"{content_hash}_{params_hash}.pkl"
    
    def get(self, documents, strategy, **kwargs):
        """ç²å–å¿«å–çµæœ"""
        cache_key = self._get_cache_key(documents, strategy, **kwargs)
        cache_path = os.path.join(self.cache_dir, cache_key)
        
        if os.path.exists(cache_path):
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        return None
    
    def set(self, documents, strategy, result, **kwargs):
        """è¨­å®šå¿«å–çµæœ"""
        cache_key = self._get_cache_key(documents, strategy, **kwargs)
        cache_path = os.path.join(self.cache_dir, cache_key)
        
        with open(cache_path, 'wb') as f:
            pickle.dump(result, f)
```

## ç¸½çµ

æœ¬æŒ‡å—æ¶µè“‹äº† RAG å‘é‡è³‡æ–™åº«æ‡‰ç”¨çš„å®Œæ•´æµç¨‹ï¼Œå¾åŸºç¤å»ºç½®åˆ°é€²éšæœ€ä½³åŒ–ã€‚é—œéµè¦é»åŒ…æ‹¬ï¼š

1. **é¸æ“‡åˆé©çš„å‘é‡è³‡æ–™åº«**ï¼šæ ¹æ“šéœ€æ±‚é¸æ“‡ Qdrantã€Weaviate æˆ–å…¶ä»–æ–¹æ¡ˆ
2. **æœ€ä½³åŒ–åˆ‡æ®µç­–ç•¥**ï¼šé€éç³»çµ±æ€§è©•ä¼°æ‰¾åˆ°æœ€é©åˆçš„æ–¹æ³•
3. **è¨­è¨ˆæœ‰æ•ˆçš„ Metadata**ï¼šæå‡æª¢ç´¢ç²¾ç¢ºåº¦å’Œå¯ç¶­è­·æ€§
4. **æŒçºŒç›£æ§å’Œæœ€ä½³åŒ–**ï¼šå»ºç«‹è©•ä¼°æ©Ÿåˆ¶ç¢ºä¿ç³»çµ±æ•ˆèƒ½

é€ééµå¾ªé€™äº›æœ€ä½³å¯¦è¸ï¼Œæ‚¨å¯ä»¥å»ºç½®å‡ºé«˜æ•ˆã€å¯é çš„ RAG ç³»çµ±ï¼Œç‚ºæ‚¨çš„æ‡‰ç”¨æä¾›å¼·å¤§çš„èªæ„æœå°‹èƒ½åŠ›ã€‚